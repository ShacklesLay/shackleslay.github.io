[{"content":"Transformer 原论文。学习于Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili。\n摘要 主流的序列转换模型（例如机器翻译）主要依赖于复杂的CNN或RNN，网络架构有encoder 和 decoder，并且有着注意力机制。 Transformer是一个简单的模型，只依赖注意力机制。 这个模型在机器翻译上的效果很好。\n结论 Transformer不同于其他模型的点是，它把被广泛运用在encoder-decoder结构上的循环层（recurrent layer）换成了多头自注意力机制。 他们觉得transformer能够在其他领域和任务上应用得很好。\n导言 在序列模型中，主流的模型是RNN中的 long short-term memory(LSTM) 和 gated recurrent (GRU)。在这其中，有两个主流的模型，语言模型和编码器-解码器的架构，后者通常被用于输出的结构化信息比较多的时候。 在RNN中，它是一个词接着一个词的输出，它们生成当前词的隐藏状态，是根据当前词的输入和前一个词的隐藏状态，这使得它能够时序化地生成。 问题在于，这样一个时序化的过程无法并行，计算性能就会比较差；另外，在生成后面的词的隐藏状态时，前面的词的隐藏状态可能会丢掉，如果不想丢掉，可能需要做一个比较大的隐藏状态，这加剧了内存开销。虽然有着很多的研究与改进，但没有解决这些问题。 Transformer纯基于注意力机制，因此可以并行，计算性能更高。\n相关工作 已经发表的很多工作意图用卷积神经网络代替循环神经网络，但是卷积神经网络很难对一个长序列建模，因为卷积每一次都只看一个很小的窗口，如果想要同时看到很远的两个位置，需要很深的卷积层才能实现。如果使用Transformer就能够很轻易的同时看到。 卷积神经网络的一个优势在于它可以有很多的输出通道，每一个输出通道都可以识别一个模式，Transformer的多头注意力机制可以模仿这个效果。 自注意机制在前人的工作中就已经提出。但transformer是第一个只基于注意力机制的编码器-解码器模型\n模型 编码器会将序列输入$(x_1,\\dots,x_n)$ （每一个$x_i$代表一个词）转换为序列$z=(z_1,\\dots, z_n)$ （每一个$z_i$是对$x_i$的向量表示） 解码器拿到$z$后，会输出序列$(y_1,\\dots,y_m)$，注意输出序列的长度与编码器的输出长度可能不一致，而且这个输出只能一个一个生成，这是一个auto-regressive（自回归）的过程，即过去时刻的输出，也会作为当前输出的输入。 Transformer也是编码器-解码器的架构，结构如下： 模型左边是编码器，右边是解码器。解码器的输入端其实是以前时刻的输出，所以名为Outputs, shifted right意为解码器输出和输入都是一个一个向右移的\n编码器和解码器 编码器 它用了N=6的六个块（原文中称为layer），每一层中有两个子层，多头自注意力机制和position-wise fully connected feed-forward network，后面这个网络其实就是MLP。每一子层再作残差链接，并作layer normalization，即 $LayerNorm(x+Sublayer(x))$。由于残差连接要求向量长度一样，因此它让每一个子层的输出维度都是 $d_{model}=512$ (这个简单设计使得Transformer相比于CNN等模型具有更少的超参数，它一般只调整上述的两个超参数)\nbatch normalization vs. Layer nomalization 考虑一个二维输入的情况，x是一个batch，shape(x)=m,n，每一行代表一个样本，每一列代表一个特征。batch norm把每一个特征在一个mini-batch里作标准化，即把每一列的均值变为0，方差变为1。而作预测时会将全局的均值和方差记录下来 同样一个二维输入，layer norm对每一行作标准化，不需要记录全局的均值和方差 transformer里的输入一般是三维的（batch, 序列，向量），此时列表示的是序列的长度。 在时序模型里，样本的长度可能不一致，如果样本的长度变化较大，由于batch norm时按照特征维度作标准化，batch norm算出来的均值和方差抖动会比较大；而layer norm由于是对每一个样本作标准化，因此没有这样的问题\n解码器 N=6的层构成，但相比于编码器里的层，解码器里多了一个子层。注意力机制的特点是，它能够一次性地看到完整的输入，但是序列生成模型，在逻辑上来所，生成当前时刻的输出时不应该看到之后的输出。为了实现这个效果，解码器使用了一个带掩码的多头注意力机制（Masked multi-head attention)\nScaled Dot-Product Attention Transformer的self-attention的相似函数是最常见的dot-product。 它的query,key的维度都等于$d_k$，value维度是$d_v$，输出是value的加权和所以和value等长。 它计算attention score(权重）的公式如下$$ Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt d_k})V$$ Q,K,V是query,key,value分别组成的矩阵\n它的这个方法和常见的dot-product注意力机制相比，多了一个scaled的步骤，即除以$\\sqrt d_k$。$d_k$很大的时候，即两个向量很长的时候，就会导致它们的运算结果在经过softmax后很接近1，而其它的权重更接近于0，此时计算梯度，会发现梯度比较小，模型训练速度很慢。由于该模型的$d_k=512$比较大，所以做一个scale. Masked 注意力机制会看到所有的输入，而masked只想要当前时刻和以前时刻的输入，因此在计算权重的时候不使用后面的值即可。 具体实现方法是，在$\\frac{QK^T}{\\sqrt d_k}$之后，对于以后时刻的输出值，将它们替换成一个很大的负数（例如-e10)，经过softmax后，对应的权重值会变成0\nMulti-Head Attention 将整个query, key, value通过投影降维 h 次，然后做h次的注意力函数，将所有函数的输出并在一起，再投影回来得到最终的输出\nQ，K，V经过线性层降维\nscaled dot-product attentino没有什么可以学习的参数，很难学习到识别不同模式的方法 而muti-head attention，线性层的w是可以学习的，也就是说，有h次机会让多头注意力在投影到的那个度量空间中学习到匹配不同模式的函数 它学习的公式如下:\n$$ \\begin{align} MultiHead(Q,K,V) = Concat(head_1, \\dots,head_h)W^o\\ where\\ head_i = Attention(QW^Q_i,KW^K_i,VW^V_i) \\end{align} $$\n原文实现中，h=8，投影之后的维度$d_k=d_v=d_{model}/h=64$.\nApplications of Attention in Transformer 编码器的输入是n x d的向量， 注意力层的输入有三个，分别是key, query, value，输出可见上述公式 解码器的输出是 n x d的向量\n解码器的掩码注意力层如前文所述，它的输出是 m x d的向量 另一个注意力层比较特殊，它的key和value来自于编码器的输出，query来自于解码器的下面的注意力层的输出。也就是说，这一层是把编码器里的输出中有用的信息拎出来，实现了编码器与解码器间的信息传递\nPosition-wise Feed Forward Networks 它就是一个MLP，特点是它把MLP对每一个词作用一次，对每一个词起作用的是同一个MLP $$ FFN(x) = max(0,xW_1+b_1)W_2+b_2 $$ x的维度是512，w1会把维度变回2048，w2把维度变回512 代码中用一维卷积来实现point-wise的效果\nAttention vs. RNN 传递序列信息 考虑最简单的情况，没有残差连接，单头attention，解码器的结构如下： attention的作用就是把整个序列中的信息抓取出来，得到的每一个输出都有它想要的信息，因此可以用MLP单独将每一个输出映射到对应的语义空间中的向量\n作为对比考虑RNN的情况，第一层是MLP，将前一个输入经过mlp得到的输出作为当前mlp的输入，如下： Embedding and Softmax embedding就是说对于每一个词，希望学习到一个向量去表示它 两个embedding层的权重以及最后一个线性层的权重是一样的，但最后embedding的权重会乘上一个$\\sqrt{d_{model}}$ ，因为如果使用 Xavier 初始化，Embedding 的方差为 1/d_model，当d_model非常大时，矩阵中的每一个值都会减小。乘上一个常数之后，在与positional encoding相加时两个的scale大小差不多\nPositional Encoding attention没有时序信息，它不会注意单词的位置，也就是说对于同一个序列，任意打乱之后再经过attention，得到的结果的顺序会变，但是值是一样的，因此要加入时序信息 positional encoding会在输入中加入时序信息，公式是sin,cos的复合函数，会生成维度与输入一致为512的，值在[-1,+1] 抖动的向量，最后加上embedding层的输出\nWhy self-attention 比较了自注意力、循环层、卷积层、构造出来的受限的自注意力的性能表现 第二列表示并行度，第三列表示得到两个点之间的关系需要的计算量 循环层和自注意力的时间复杂度是差不多的，由于是时序性地，因此序列操作是O(n)，同样，传递信息也是时序性的 卷积的时间复杂度和循环层差不多，一次卷积就能完成，每一次检查信息是通过一个宽度为k的窗口看，如果两个点不在同一个窗口内，就要多层才能看到两个点的信息，因此是$O(log_k(n))$ 受限的自注意力，query只跟r个邻居去计算\n由上表的比较可知，self-attention的性能看起来更好\n但在实际中，由于self-attention对模型的假设做得更少，因此要用更大的模型和更多的数据才能训练出来和CNN和RNN同样的效果，所以基于Transformer的模型一般都很贵\nTraning 暂时不需要了解训练细节\n代码实现 数据预处理时，由于输入的序列长度不一，因此需要做截断和填充操作。这样使得一个batch的序列长度一致，便于并行处理。 当把这些经过填充后的序列喂给编码器时，需要实现一个函数来告知哪一些词元是填充词元，以免这些填充词元参与attention weight的计算 可见视频2.Decoder代码解读_哔哩哔哩_bilibili\n","permalink":"https://shackleslay.github.io/posts/third/","summary":"Transformer 原论文。学习于Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili。 摘要 主流的序列转换模型（例如机器翻译）主要依赖于复杂的CNN或RNN，网络架构有encoder 和 decoder，并且有着注意力机制。 Transformer是一个简单的模型，只依赖注意力机制","title":"Third"},{"content":"根因 BERT的位置编码使用的是可训练的绝对位置编码，一般最大位置设为了512，不具有外推性； 注意力机制的计算复杂度为$O(n^2)$，长序列显存用量大大增加。 模型角度 考虑消除BERT的长度限制，把position_embedding层的大小变大，前512个向量用原来的向量初始化，其余的可以随机初始化，也可以用与前512个向量相同的向量来初始化 考虑使用其他输入长度更长的模型 更换位置编码，但这一般意味着需要从头开始预训练 文本角度 截断法：只选择文本的部分内容作为输入（根据具体模型选择输入的长度，例如BERT一般选择512个token）。值得注意的是，选择文本的部分内容时，不同的数据集有不同的表现。有的数据集可能选择前512个token效果最好，而有的数据集可能选择最后512个token最好，还有的数据集可能是在前面和后面都选择一部分token，根据具体任务选择不同的截断方法； 压缩法：用某种策略对文本进行压缩 滑动窗口法：使用滑动窗口对文本进行有重叠的分割，分割得到的每个片段都是单个的训练样本，具有相同的标签。推理的时候也对输入进行分割，用投票法得到最后的推理结果 池化法：对文本进行分割得到多个文本片段，放在新的维度上，将它们输入BERT，然后在新维度上进行池化。池化法有 max pooling, avg pooling, simple attention, transformer等 冻结法：对文本进行分割，然后用参数冻结的BERT得到每个文本片段的表示，再送入别的网络架构（例如LSTM+FC）进行训练 参考： nlp - How to use Bert for long text classification? - Stack Overflow Plans to support longer sequences? · Issue #27 · google-research/bert (github.com) 【关于BERT如何处理篇章级长文本】那些你不知道的事 - 知乎 (zhihu.com) ​实践中BERT如何对长度大于500的文本进行处理？ - 知乎 (zhihu.com\n","permalink":"https://shackleslay.github.io/posts/second-post/","summary":"根因 BERT的位置编码使用的是可训练的绝对位置编码，一般最大位置设为了512，不具有外推性； 注意力机制的计算复杂度为$O(n^2)$，长序列显存用量大大增加。 模型角度 考虑消除BERT的长度限制，把position_embedding层的大小变大，前512个向量用原来的向量初始化，","title":"Second Post"},{"content":"原博客链接：Assisted Generation: a new direction toward low-latency text generation (huggingface.co) 本文使用ChatGPT翻译，本人做简单润色。 简单总结下文章： Transformers给它们的API加了个新功能叫做辅助生成。这个方法需要用到两个大模型，一个是助手模型，一个是本体模型。具体流程是用一个小规模模型（例如量化模型）来充当助手，生成文本时，先由助手来进行生成，然后把生成的文本交给本体模型，它只需要跑一个前向过程来更正助手模型生成的文本，然后将更正的文本返回助手模型继续生成。不理解这个流程的可以看下面的动图。 除了这个新功能以外，文章还介绍了文本生成延迟的基本概念和一些解决方案，也值得一读。 ——————————————————\n大型语言模型现在非常流行，许多公司投入了大量资源来扩展它们并解锁新的能力。然而，作为越来越难以集中注意力的人类，我们也不喜欢它们的响应速度缓慢。延迟对于良好的用户体验非常关键，因此即使质量较低（例如在代码完成中），较小的模型也经常被使用。\n为什么文本生成如此缓慢？是什么阻止你在不破产的情况下部署低延迟的大型语言模型？在这篇博客文章中，我们将重新审视自回归文本生成的瓶颈，并介绍一种新的解码方法来解决延迟问题。您将看到，通过使用我们的新方法——辅助生成，您可以在通用硬件上将延迟降低多达10倍！\n理解文本生成延迟 现代文本生成的核心很容易理解。让我们看看中心部分——机器学习模型。它的输入包含一个文本序列，其中包括迄今为止生成的文本，以及潜在的其他模型特定组件（例如，Whisper还有音频输入）。模型接受输入并运行前向传递：输入被馈送到模型中，并依次通过其层，直到预测出下一个标记的未归一化对数概率（也称为logits）。标记可能由整个单词、子单词或甚至单个字符组成，具体取决于模型。 模型前向传递可以得到下一个标记的logits，您可以自由操纵这些logits（例如，将不良单词或序列的概率设置为0）。文本生成的下一步是从这些logits中选择下一个标记。常见的策略包括选择最可能的标记，称为贪心解码，或从它们的分布中进行采样，也称为多项式采样。通过将模型前向传递与下一个标记选择迭代地结合起来，就可以得到文本生成。\n从上面的描述可以看出，文本生成中的延迟瓶颈很明显：对于大型模型来说，运行模型前向传递是很慢的，而且您可能需要按顺序进行数百次前向传递。但让我们深入探讨一下：为什么前向传递很慢？前向传递通常由矩阵乘法主导，并且在快速访问相应的维基百科部分后，您可以发现内存带宽是此操作的限制因素（例如从GPU RAM到GPU计算核心）。换句话说，在前向传递中的瓶颈来自于将模型层的权重加载到设备的计算核心中，而不是来自于执行计算本身。\n目前，您有三种主要方法可以探索，以充分利用文本生成，所有这些方法都解决模型前向传递的性能问题。首先，您可以使用特定于硬件的模型优化。例如，您的设备可能与Flash Attention兼容，该技术通过重新排列操作来加速注意力层，或者使用INT8量化，可以减小模型权重的大小。\n其次，当您知道会有并发的文本生成请求时，您可以批量处理输入，并在略微增加延迟的情况下大大提高吞吐量。现在，加载到设备中的模型层权重将同时用于多个输入行，这意味着您将获得更多的标记，而负担的内存带宽大致相同。批处理的问题在于您需要额外的设备内存（或将内存转移到其他地方）。\n# 展示批处理的威力. 设备: RTX3090 from transformers import AutoModelForCausalLM, AutoTokenizer import time tokenizer = AutoTokenizer.from_pretrained(\u0026#34;distilgpt2\u0026#34;) model = AutoModelForCausalLM.from_pretrained(\u0026#34;distilgpt2\u0026#34;).to(\u0026#34;cuda\u0026#34;) inputs = tokenizer([\u0026#34;Hello world\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;).to(\u0026#34;cuda\u0026#34;) def print_tokens_per_second(batch_size): new_tokens = 100 cumulative_time = 0 # warmup model.generate( **inputs, do_sample=True, max_new_tokens=new_tokens, num_return_sequences=batch_size ) for _ in range(10): start = time.time() model.generate( **inputs, do_sample=True, max_new_tokens=new_tokens, num_return_sequences=batch_size ) cumulative_time += time.time() - start print(f\u0026#34;Tokens per second: {new_tokens * batch_size * 10 / cumulative_time:.1f}\u0026#34;) print_tokens_per_second(1) # Tokens per second: 418.3 print_tokens_per_second(64) # Tokens per second: 16266.2 (~39x more tokens per second) 最后，如果您有多个可用设备，您可以使用Tensor Parallelism分配工作负载并获得较低的延迟。使用Tensor Parallelism，您将内存带宽负担分配到多个设备上，但现在您还必须考虑设备之间的通信瓶颈，以及运行多个设备的货币成本。其好处在很大程度上取决于模型大小：容易适应单个消费设备的模型受益有限。从这篇[DeepSpeed博客](DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression - Microsoft Research)文章中可以看出，您可以将一个170亿参数模型分布在4个GPU上，将延迟降低1.5倍（图7）。\n这三种改进方法可以结合使用，从而实现高吞吐量的解决方案。然而，在应用特定于硬件的优化之后，减少延迟的选项有限，并且现有的选项非常昂贵。让我们解决这个问题！\n重新审视前向传递 你已经了解到每个模型的前向传递都会产生下一个标记的对数概率，但这实际上是不完整的描述。在文本生成过程中，典型的迭代方式是将最新生成的标记作为输入传递给模型，并携带了所有其他先前输入的缓存内部计算结果，返回下一个标记的对数概率。缓存用于避免冗余计算，从而导致更快的前向传递，但它并不是强制性的（可以部分使用）。当禁用缓存时，输入包含迄今为止生成的所有标记序列，输出包含序列中所有位置的下一个标记对应的对数概率! 位置N处的对数概率对应于下一个标记的分布，如果输入由前N个标记组成，则忽略序列中所有后续标记。在贪婪解码的特殊情况下，如果将生成的序列作为输入传递并对结果的对数概率应用argmax运算符，则会获得生成的序列。\nfrom transformers import AutoModelForCausalLM, AutoTokenizer tok = AutoTokenizer.from_pretrained(\u0026#34;distilgpt2\u0026#34;) model = AutoModelForCausalLM.from_pretrained(\u0026#34;distilgpt2\u0026#34;) inputs = tok([\u0026#34;The\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) generated = model.generate(**inputs, do_sample=False, max_new_tokens=10) forward_confirmation = model(generated).logits.argmax(-1) # We exclude the opposing tips from each sequence: the forward pass returns # the logits for the next token, so it is shifted by one position. print(generated[:-1].tolist() == forward_confirmation[1:].tolist()) # True 这意味着你可以将模型的前向传递用于不同的目的：除了提供一些标记以预测下一个标记外，还可以将一个序列传递给模型，并检查模型是否会生成相同的序列（或其一部分）。\n假设您可以访问一个魔法的无延迟预测模型，该模型对于任何给定的输入都生成与您的模型相同的序列。为了论证，它不能直接使用，而是被限制为辅助您的生成过程。利用上述描述的属性，您可以使用此辅助模型获取候选输出标记，然后使用您的模型进行前向传递以确认它们确实是正确的。在这个理想的场景中，文本生成的延迟将从O(n)降至O(1)，其中n是生成的标记数量。对于长时间的生成，我们谈论的是几个数量级的巨大差距。\n接近现实的情况是，假设辅助模型失去了其预测所有标记的能力。现在它是一个无延迟的模型，根据您的模型，它会将一些候选标记标记为错误。由于任务的自回归性质，一旦辅助模型错了一个标记，所有后续的候选标记都必须无效。但这并不妨碍您在使用您的模型更正错误标记后再次查询助手，反复迭代此过程。即使助手偶尔错了几个标记，文本生成的延迟也比原始形式少一个数量级。\n显然，不存在无延迟的辅助模型。然而，相对容易地找到一个模型，该模型近似于另一个模型的文本生成输出 - 相同架构的较小版本经过类似的训练通常符合这个属性。此外，当模型大小的差异变得显著时，使用较小的模型作为辅助模型的成本将成为事后的想法，考虑到跳过几个前向传递所带来的好处！现在，您已经理解了辅助生成的核心。\n使用辅助生成的贪婪解码 辅助生成是一个平衡的过程。您希望助手能够快速生成候选序列，同时尽可能准确。如果助手的质量很差，使用辅助模型的成本就会很高，而且几乎没有好处。另一方面，优化候选序列的质量可能意味着使用缓慢的助手，导致净减速。虽然我们无法为您自动选择助手模型，但我们已经包含了一个附加要求和一个启发式规则，以确保与助手一起花费的时间得到控制。\n首先是要求 - 助手必须与您的模型具有完全相同的分词器。如果没有这个要求，就必须添加昂贵的标记解码和重新编码步骤。此外，这些额外步骤必须在CPU上进行，这反过来可能需要缓慢的设备间数据传输。快速使用助手对于辅助生成的好处至关重要。\n最后是启发式规则。到这个地步，你可能已经注意到了电影《盗梦空间》和辅助生成之间的相似之处 - 你毕竟是在文本生成中运行文本生成。每个候选标记都将有一个助手模型的前向传递，我们知道前向传递是昂贵的。虽然您无法提前知道助手模型将正确地得到多少个标记，但可以跟踪此信息并将其用于限制请求助手的候选标记的数量 - 输出的某些部分比其他部分更容易预测。\n最后，这是我们辅助生成循环的原始实现（代码）：\n使用贪婪解码和辅助模型来生成一定数量的候选标记。产生的候选标记数量在第一次辅助生成调用时初始化为.candidates5。 使用我们的模型进行前向传递，得到.candidateslogits 使用标记选择方法（贪心搜索使用argmax()，采样使用multinomial()），从logits获取下一个标记next_tokenslogits。 将其与之前生成的候选标记进行比较并获取匹配的标记数。请记住，必须以从左到右的因果关系进行比较：在第一个不匹配之后，所有候选都将无效。 使用匹配的标记数来切片并丢弃与未确认候选标记相关的变量。实质上，在next_tokens中，保留匹配的标记以及第一个不同的标记（我们的模型从有效的候选子序列中生成）。 调整下一次迭代中要产生的候选标记数 - 我们的原始启发式规则如果所有标记都匹配，则将其增加1，否则将其减少1。 video\n我们在🤗Transformers中设计了API，使此过程对您来说非常轻松。您只需要将助手模型传递给新的关键字参数，即可获得延迟的收益！在发布本博客文章时，辅助生成仅限于批处理大小为1。\nfrom transformers import AutoModelForCausalLM, AutoTokenizer import torch prompt = \u0026#34;Alice and Bob\u0026#34; checkpoint = \u0026#34;EleutherAI/pythia-1.4b-deduped\u0026#34; assistant_checkpoint = \u0026#34;EleutherAI/pythia-160m-deduped\u0026#34; device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) inputs = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).to(device) model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device) assistant_model=AutoModelForCausalLM.from_pretrained(assistant_checkpoint).to(device) outputs = model.generate(**inputs, assistant_model=assistant_model) print(tokenizer.batch_decode(outputs, skip_special_tokens=True)) # [\u0026#39;Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a\u0026#39;] 额外的内部复杂性是否值得？让我们看看贪婪解码情况下的延迟数字（采样结果在下一节中），考虑批处理大小为1。这些结果是直接从🤗Transformers中提取的，没有进行任何额外的优化，因此您应该能够在您的设置中重现它们。 从收集的数字中可以看出，辅助生成可以在不同的设置中实现显著的延迟降低，但它并不是万能的 - 在应用于您的用例之前，应该进行基准测试。我们可以得出结论，辅助生成：\n🤏 需要访问一个比您的模型小至少一个数量级的助手模型（差异越大，越好）；\n🚀 在存在INT8时，速度可提高3倍，在GPU内存容量足够时最多可提高2倍；\n🤯 如果您在处理无法适应GPU而依靠内存卸载的模型，则可以看到高达10倍的加速；\n📄 在输入受限任务（如自动语音识别或摘要）中表现出色。\n辅助生成采样 贪婪解码适用于输入受限任务（自动语音识别、翻译、摘要等）或寻求事实知识的任务。需要大量创造力的开放性任务，例如将语言模型用作聊天机器人的大多数用途，应该使用采样。辅助生成自然设计用于贪心解码，但这并不意味着您不能使用辅助生成进行多项式采样！\n从概率分布中抽取下一个标记的样本将导致我们的贪婪助手更容易失败，从而降低其延迟优势。然而，我们可以使用在大多数基于采样的应用程序中存在的温度系数来控制下一个标记的概率分布的锐度。在一个极端情况下，温度接近0时，采样将近似于贪婪解码，偏向于最可能的标记。在另一个极端情况下，将温度设置为远大于1的值，采样将是混沌的，并从均匀分布中抽取。因此，低温度更有利于您的助手模型，保留了辅助生成的大部分延迟优势，如下所示。\n未来方向 辅助生成表明，现代文本生成策略正成熟可用于优化。了解它目前是一个内存限制问题，而不是计算限制问题，使我们能够应用简单的启发式方法，充分利用可用的内存带宽，缓解瓶颈。我们相信，进一步改进使用助手模型的方法将带来更大的延迟降低 - 例如，如果我们要求助手生成多个候选延续，我们可能能够跳过更多前向传递。自然地，发布高质量的小型模型以用作助手，将对实现和放大好处至关重要。\n最初在我们的🤗Transformers库中发布，用于generate()函数，我们希望在整个Hugging Face宇宙中提供它。它的实现也是完全开源的，因此，如果您正在从事文本生成而不使用我们的工具，请随意将其用作参考。\n最后，辅助生成重新引出了文本生成中的一个关键问题。在给定模型的情况下，所有新标记都是固定计算量的结果，该领域一直在这样的约束下发展。在纯自回归方式下，每次同质前向传递生成一个标记。本博客文章强调了这不应该是这种情况：生成的输出的大部分也可以由尺寸小得多的模型同样生成。为此，我们将需要新的模型架构和解码方法 - 我们非常期待未来的发展！\n","permalink":"https://shackleslay.github.io/posts/my-first-post/","summary":"原博客链接：Assisted Generation: a new direction toward low-latency text generation (huggingface.co) 本文使用ChatGPT翻译，本人做简单润色。 简单总结下文章： Transformers给它们的API加了个新功能叫做辅助生成。这个方法需要用到两个大模型，一个是助手模型，一个是本体模型。具体流程是用一个小规模模型（例如量化模型）来充当助","title":"My First Post"}]